{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "# Task 1.2: Handling PlayerID Updates Across Related Tables\n",
        "\n",
        "## Executive Summary\n",
        "\n",
        "This notebook addresses the critical data architecture challenge of maintaining referential integrity when PlayerIDs are merged or updated across a multi-table database schema. We present both the theoretical framework and our practical implementation, demonstrating how cascading updates ensure data consistency in a production environment.\n",
        "\n",
        "**Objective**: Outline a robust, repeatable approach to handle PlayerID changes that maintains accuracy across all related tables (Players, Reports, Contracts, Appearances, etc.) while ensuring the process is consistent and repeatable for future data updates.\n",
        "\n",
        "**Requirements from PDF**:\n",
        "- How to identify which tables or datasets are affected by PlayerID changes\n",
        "- How to ensure that relationships between tables remain correct after updates or merges\n",
        "- Steps to make this process consistent and repeatable for future data updates\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Import required libraries\n",
        "import pandas as pd\n",
        "import json\n",
        "from pathlib import Path\n",
        "import sys\n",
        "\n",
        "# Add src to path\n",
        "sys.path.insert(0, str(Path('..') / 'src'))\n",
        "\n",
        "# Set display options\n",
        "pd.set_option('display.max_columns', None)\n",
        "pd.set_option('display.max_rows', 50)\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## Part 1: Theoretical Foundation - Cascading Updates in Relational Databases\n",
        "\n",
        "### 1.1 Understanding Cascading Updates\n",
        "\n",
        "**Cascading Updates** are a fundamental database constraint mechanism that automatically propagates changes from a primary key (or unique identifier) in a parent table to all corresponding foreign keys in child tables. This ensures referential integrity is maintained without manual intervention.\n",
        "\n",
        "**Key Concepts**:\n",
        "\n",
        "1. **Primary Key (Parent Table)**: The `Players` table contains the master record with `PlayerID` as the primary key\n",
        "2. **Foreign Key (Child Tables)**: Related tables (`Reports`, `Contracts`, `Appearances`) reference `PlayerID` as a foreign key\n",
        "3. **Cascade Behavior**: When a `PlayerID` is updated or merged in the parent table, all child table references are automatically updated\n",
        "\n",
        "**Database-Level Implementation** (SQL):\n",
        "```sql\n",
        "-- Example: Foreign key constraint with CASCADE UPDATE\n",
        "ALTER TABLE Reports\n",
        "ADD CONSTRAINT fk_player\n",
        "FOREIGN KEY (PlayerID) \n",
        "REFERENCES Players(PlayerID)\n",
        "ON UPDATE CASCADE;\n",
        "```\n",
        "\n",
        "**Why This Matters**:\n",
        "- **Data Integrity**: Prevents orphaned records (reports without valid player references)\n",
        "- **Consistency**: Ensures all tables always reference the correct, canonical PlayerID\n",
        "- **Automation**: Eliminates manual, error-prone update processes\n",
        "- **Atomicity**: Changes happen transactionally - all or nothing\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "### 1.2 Our Implementation: Translation Lookup Table Pattern\n",
        "\n",
        "In our ETL pipeline, we implement cascading updates using a **Translation Lookup Table** pattern, which is the industry-standard approach for data migration and ID reconciliation in data warehouses and analytics platforms.\n",
        "\n",
        "**Architecture**:\n",
        "\n",
        "```\n",
        "┌─────────────────┐\n",
        "│  Players (Raw)  │\n",
        "│  - PLY_ABC123   │  ──┐\n",
        "│  - PLY_XYZ789   │    │  Duplicate Detection\n",
        "│  - PLY_ABC123   │  ──┘  & Canonical ID Selection\n",
        "└─────────────────┘\n",
        "         │\n",
        "         ▼\n",
        "┌─────────────────────────┐\n",
        "│  player_id_map.json     │  ← Translation Lookup Table\n",
        "│  {                      │\n",
        "│    \"PLY_XYZ789\":        │\n",
        "│      \"PLY_ABC123\"       │  ← Old ID → Canonical ID\n",
        "│  }                      │\n",
        "└─────────────────────────┘\n",
        "         │\n",
        "         ├──────────────────┐\n",
        "         ▼                  ▼\n",
        "┌─────────────────┐  ┌──────────────────┐\n",
        "│ Players (Clean) │  │ Reports (Clean)  │\n",
        "│ - PLY_ABC123    │  │ - PLY_ABC123     │  ← All IDs updated\n",
        "└─────────────────┘  └──────────────────┘\n",
        "```\n",
        "\n",
        "**Key Advantages**:\n",
        "1. **Auditability**: Complete change log of all ID mappings\n",
        "2. **Reversibility**: Can trace back from canonical ID to original IDs\n",
        "3. **Incremental Processing**: Can apply mappings to new data without reprocessing entire datasets\n",
        "4. **Version Control**: JSON format allows tracking changes over time\n",
        "5. **Cross-Platform**: Works with CSV, databases, APIs, and data warehouses\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Load our actual ID mapping to demonstrate the pattern\n",
        "BASE_DIR = Path('..')\n",
        "mapping_path = BASE_DIR / 'data' / 'processed' / 'player_id_map.json'\n",
        "\n",
        "with open(mapping_path, 'r') as f:\n",
        "    id_mapping = json.load(f)\n",
        "\n",
        "print(\"=\" * 80)\n",
        "print(\"TRANSLATION LOOKUP TABLE (player_id_map.json)\")\n",
        "print(\"=\" * 80)\n",
        "print(f\"\\nTotal ID Mappings: {len(id_mapping)}\")\n",
        "print(\"\\nSample Mappings (showing different scenarios):\")\n",
        "print(\"-\" * 80)\n",
        "\n",
        "# Show examples\n",
        "sample_mappings = list(id_mapping.items())[:10]\n",
        "for old_id, new_id in sample_mappings:\n",
        "    print(f\"  {old_id:25s} → {new_id}\")\n",
        "\n",
        "print(\"\\n\" + \"=\" * 80)\n",
        "print(\"MAPPING STATISTICS\")\n",
        "print(\"=\" * 80)\n",
        "\n",
        "# Analyze mapping patterns\n",
        "mapping_types = {\n",
        "    'Same ID with suffix': sum(1 for old, new in id_mapping.items() if new.startswith(old + '_')),\n",
        "    'Different ID': sum(1 for old, new in id_mapping.items() if not new.startswith(old + '_')),\n",
        "    'Total': len(id_mapping)\n",
        "}\n",
        "\n",
        "for mapping_type, count in mapping_types.items():\n",
        "    print(f\"  {mapping_type}: {count}\")\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## Part 2: Answering the Three Core Requirements\n",
        "\n",
        "### 2.1 Requirement 1: How to Identify Which Tables Are Affected\n",
        "\n",
        "**Theoretical Approach**:\n",
        "\n",
        "In a relational database, we identify affected tables through **Foreign Key Dependencies**:\n",
        "\n",
        "1. **Direct Dependencies**: Tables with explicit foreign key constraints referencing `Players.PlayerID`\n",
        "2. **Indirect Dependencies**: Tables that reference other tables which in turn reference `Players.PlayerID`\n",
        "3. **Application-Level Dependencies**: Code/APIs that use PlayerID for lookups or joins\n",
        "\n",
        "**SQL-Based Discovery**:\n",
        "```sql\n",
        "-- Find all tables with foreign keys to Players\n",
        "SELECT \n",
        "    TABLE_NAME,\n",
        "    COLUMN_NAME,\n",
        "    CONSTRAINT_NAME\n",
        "FROM INFORMATION_SCHEMA.KEY_COLUMN_USAGE\n",
        "WHERE REFERENCED_TABLE_NAME = 'Players'\n",
        "AND REFERENCED_COLUMN_NAME = 'PlayerID';\n",
        "```\n",
        "\n",
        "**Our Practical Implementation**:\n",
        "\n",
        "In our current dataset, we have identified the following affected tables:\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Identify affected tables by examining our data structure\n",
        "BASE_DIR = Path('..')\n",
        "\n",
        "# Check what tables/files we have\n",
        "data_raw = BASE_DIR / 'data' / 'raw'\n",
        "data_processed = BASE_DIR / 'data' / 'processed'\n",
        "\n",
        "print(\"=\" * 80)\n",
        "print(\"AFFECTED TABLES/DATASETS IDENTIFICATION\")\n",
        "print(\"=\" * 80)\n",
        "\n",
        "# Load Players to see structure\n",
        "players_path = data_processed / 'players_cleaned.csv'\n",
        "reporting_path = data_processed / 'reporting_cleaned.csv'\n",
        "\n",
        "df_players = pd.read_csv(players_path)\n",
        "df_reporting = pd.read_csv(reporting_path)\n",
        "\n",
        "print(\"\\n1. PRIMARY TABLE (Parent):\")\n",
        "print(f\"   - Players.csv\")\n",
        "print(f\"     • Primary Key: PlayerID\")\n",
        "print(f\"     • Records: {len(df_players)}\")\n",
        "print(f\"     • Unique PlayerIDs: {df_players['PlayerID'].nunique()}\")\n",
        "\n",
        "print(\"\\n2. CHILD TABLES (Foreign Key Dependencies):\")\n",
        "print(f\"   - ReportingInsight.csv\")\n",
        "print(f\"     • Foreign Key: PlayerID (references Players.PlayerID)\")\n",
        "print(f\"     • Records: {len(df_reporting)}\")\n",
        "print(f\"     • Unique PlayerIDs referenced: {df_reporting['PlayerID'].nunique()}\")\n",
        "\n",
        "# Check for orphaned references\n",
        "players_ids = set(df_players['PlayerID'].unique())\n",
        "reporting_ids = set(df_reporting['PlayerID'].dropna().unique())\n",
        "orphaned = reporting_ids - players_ids\n",
        "\n",
        "print(f\"\\n3. REFERENTIAL INTEGRITY CHECK:\")\n",
        "print(f\"   • PlayerIDs in Reporting: {len(reporting_ids)}\")\n",
        "print(f\"   • PlayerIDs in Players: {len(players_ids)}\")\n",
        "print(f\"   • Orphaned PlayerIDs: {len(orphaned)}\")\n",
        "if len(orphaned) == 0:\n",
        "    print(f\"   ✅ Status: PASS - All references valid\")\n",
        "else:\n",
        "    print(f\"   ⚠️  Status: WARNING - {len(orphaned)} orphaned references found\")\n",
        "\n",
        "print(\"\\n4. POTENTIAL FUTURE TABLES (Not in current dataset but expected in production):\")\n",
        "print(\"   - Contracts (ContractID, PlayerID, StartDate, EndDate, Salary)\")\n",
        "print(\"   - Appearances (AppearanceID, PlayerID, MatchID, Minutes, Goals)\")\n",
        "print(\"   - Injuries (InjuryID, PlayerID, InjuryType, RecoveryDate)\")\n",
        "print(\"   - Transfers (TransferID, PlayerID, FromClub, ToClub, TransferFee)\")\n",
        "print(\"   - Medical Records (MedicalID, PlayerID, ExaminationDate, Status)\")\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "### 2.2 Requirement 2: Ensuring Relationships Remain Correct\n",
        "\n",
        "**Theoretical Framework**:\n",
        "\n",
        "Maintaining referential integrity requires three critical mechanisms:\n",
        "\n",
        "#### A. Foreign Key Constraints\n",
        "\n",
        "**Database-Level** (Production SQL Database):\n",
        "```sql\n",
        "-- Enforce referential integrity at database level\n",
        "ALTER TABLE Reports\n",
        "ADD CONSTRAINT fk_reports_player\n",
        "FOREIGN KEY (PlayerID) \n",
        "REFERENCES Players(PlayerID)\n",
        "ON UPDATE CASCADE\n",
        "ON DELETE RESTRICT;\n",
        "```\n",
        "\n",
        "**ETL-Level** (Our Current Implementation):\n",
        "- Validation step after ID mapping application\n",
        "- Orphaned record detection and resolution\n",
        "- Transactional processing (all-or-nothing updates)\n",
        "\n",
        "#### B. Transaction Atomicity\n",
        "\n",
        "All ID updates must occur within a single transaction to ensure:\n",
        "- **Consistency**: Either all tables are updated, or none are\n",
        "- **Isolation**: No partial updates visible to other processes\n",
        "- **Durability**: Changes are permanent once committed\n",
        "\n",
        "#### C. Validation and Verification\n",
        "\n",
        "Post-update validation ensures:\n",
        "1. All foreign key references point to valid PlayerIDs\n",
        "2. No orphaned records exist\n",
        "3. Mapping completeness (all old IDs have corresponding new IDs)\n",
        "\n",
        "**Our Implementation**:\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Demonstrate our referential integrity validation\n",
        "from clean_reporting import validate_referential_integrity\n",
        "\n",
        "# Load cleaned data\n",
        "df_players_clean = pd.read_csv(players_path)\n",
        "df_reporting_clean = pd.read_csv(reporting_path)\n",
        "\n",
        "# Perform validation\n",
        "integrity_check = validate_referential_integrity(df_reporting_clean, df_players_clean)\n",
        "\n",
        "print(\"=\" * 80)\n",
        "print(\"REFERENTIAL INTEGRITY VALIDATION\")\n",
        "print(\"=\" * 80)\n",
        "print(f\"\\nStatus: {integrity_check['integrity_status']}\")\n",
        "print(f\"Unique PlayerIDs in Reporting: {integrity_check['total_unique_player_ids_in_reporting']}\")\n",
        "print(f\"Unique PlayerIDs in Players: {integrity_check['total_unique_player_ids_in_players']}\")\n",
        "print(f\"Orphaned PlayerIDs: {integrity_check['orphaned_count']}\")\n",
        "print(f\"Orphaned Rows: {integrity_check['orphaned_rows']}\")\n",
        "\n",
        "if integrity_check['integrity_status'] == 'PASS':\n",
        "    print(\"\\n✅ SUCCESS: All relationships maintained correctly!\")\n",
        "    print(\"   Every PlayerID in ReportingInsight exists in Players table.\")\n",
        "else:\n",
        "    print(f\"\\n⚠️  WARNING: {integrity_check['orphaned_count']} orphaned references detected\")\n",
        "    if integrity_check['orphaned_player_ids']:\n",
        "        print(\"   Orphaned IDs:\", integrity_check['orphaned_player_ids'][:10])\n",
        "\n",
        "# Show how ID mapping was applied\n",
        "print(\"\\n\" + \"=\" * 80)\n",
        "print(\"ID MAPPING APPLICATION PROCESS\")\n",
        "print(\"=\" * 80)\n",
        "\n",
        "# Count how many reports were affected by ID mappings\n",
        "mapped_ids_in_reporting = set(id_mapping.values())\n",
        "reports_with_mapped_ids = df_reporting_clean[df_reporting_clean['PlayerID'].isin(mapped_ids_in_reporting)]\n",
        "\n",
        "print(f\"\\nReports affected by ID mappings: {len(reports_with_mapped_ids)}\")\n",
        "print(f\"Percentage of total reports: {len(reports_with_mapped_ids) / len(df_reporting_clean) * 100:.1f}%\")\n",
        "\n",
        "# Show example\n",
        "if len(reports_with_mapped_ids) > 0:\n",
        "    print(\"\\nExample: Reports with mapped PlayerIDs:\")\n",
        "    example = reports_with_mapped_ids[['PlayerID', 'PlayerName', 'PerformanceGrade', 'PotentialGrade']].head(5)\n",
        "    print(example.to_string(index=False))\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "### 2.3 Requirement 3: Making the Process Repeatable and Consistent\n",
        "\n",
        "**Architecture Principles for Repeatability**:\n",
        "\n",
        "#### A. ETL Pipeline Automation\n",
        "\n",
        "Our pipeline follows the **Extract-Transform-Load (ETL)** pattern with these characteristics:\n",
        "\n",
        "1. **Idempotency**: Running the pipeline multiple times produces the same result\n",
        "2. **Incremental Processing**: Can handle new data without reprocessing entire datasets\n",
        "3. **Version Control**: ID mappings are versioned and auditable\n",
        "4. **Error Handling**: Graceful failure with rollback capabilities\n",
        "\n",
        "#### B. Logging and Audit Trail\n",
        "\n",
        "Every ID change is logged with:\n",
        "- **Timestamp**: When the change occurred\n",
        "- **Source**: Which process/script made the change\n",
        "- **Reason**: Why the ID was changed (duplicate detection, data quality, etc.)\n",
        "- **Impact**: How many records were affected\n",
        "\n",
        "#### C. Configuration Management\n",
        "\n",
        "The process is driven by configuration, not hardcoded logic:\n",
        "- Mapping rules are data-driven (stored in JSON)\n",
        "- Cleaning rules are parameterized\n",
        "- Validation thresholds are configurable\n",
        "\n",
        "**Our Implementation Structure**:\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Demonstrate the repeatable pipeline structure\n",
        "print(\"=\" * 80)\n",
        "print(\"REPEATABLE PIPELINE ARCHITECTURE\")\n",
        "print(\"=\" * 80)\n",
        "\n",
        "print(\"\\n1. PIPELINE ORCHESTRATION (src/pipeline.py):\")\n",
        "print(\"   ├── Step 1: Clean Players Data\")\n",
        "print(\"   │   └── Generates: players_cleaned.csv + player_id_map.json\")\n",
        "print(\"   │\")\n",
        "print(\"   ├── Step 2: Clean Reporting Data\")\n",
        "print(\"   │   ├── Loads: player_id_map.json\")\n",
        "print(\"   │   ├── Applies: ID mappings to ReportingInsight\")\n",
        "print(\"   │   └── Generates: reporting_cleaned.csv\")\n",
        "print(\"   │\")\n",
        "print(\"   └── Step 3: Generate Audit Log\")\n",
        "print(\"       └── Generates: cleaning_log.md (before/after metrics)\")\n",
        "\n",
        "print(\"\\n2. ID MAPPING APPLICATION (src/clean_reporting.py):\")\n",
        "print(\"   - Function: apply_id_mapping()\")\n",
        "print(\"   - Input: Reporting DataFrame + ID Mapping Dictionary\")\n",
        "print(\"   - Process: Lookup and replace old IDs with canonical IDs\")\n",
        "print(\"   - Output: Updated Reporting DataFrame with mapped IDs\")\n",
        "\n",
        "# Show the actual mapping application code pattern\n",
        "print(\"\\n3. CODE PATTERN (Pseudocode):\")\n",
        "print(\"\"\"\n",
        "def apply_id_mapping(df_reporting, id_mapping):\n",
        "    # Create lookup function\n",
        "    def map_player_id(player_id):\n",
        "        if player_id in id_mapping:\n",
        "            return id_mapping[player_id]  # Return canonical ID\n",
        "        return player_id  # Keep original if not in mapping\n",
        "    \n",
        "    # Apply mapping to all rows\n",
        "    df_reporting['PlayerID'] = df_reporting['PlayerID'].apply(map_player_id)\n",
        "    \n",
        "    # Return updated dataframe\n",
        "    return df_reporting\n",
        "\"\"\")\n",
        "\n",
        "print(\"\\n4. VALIDATION CHECKPOINTS:\")\n",
        "print(\"   ✅ Pre-processing: Validate input data structure\")\n",
        "print(\"   ✅ Post-mapping: Verify all IDs exist in Players table\")\n",
        "print(\"   ✅ Final: Generate integrity report\")\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## Part 3: Production-Ready Implementation Strategy\n",
        "\n",
        "### 3.1 Database Migration Strategy (For Production SQL Database)\n",
        "\n",
        "When moving from CSV-based ETL to a production database, the implementation would follow this pattern:\n",
        "\n",
        "**Phase 1: Preparation**\n",
        "```sql\n",
        "-- Create staging table for ID mappings\n",
        "CREATE TABLE player_id_mappings (\n",
        "    old_player_id VARCHAR(50) PRIMARY KEY,\n",
        "    new_player_id VARCHAR(50) NOT NULL,\n",
        "    mapping_reason VARCHAR(255),\n",
        "    created_at TIMESTAMP DEFAULT CURRENT_TIMESTAMP,\n",
        "    INDEX idx_new_id (new_player_id)\n",
        ");\n",
        "```\n",
        "\n",
        "**Phase 2: Apply Mappings**\n",
        "```sql\n",
        "-- Update Reports table using JOIN with mapping table\n",
        "UPDATE Reports r\n",
        "INNER JOIN player_id_mappings m ON r.PlayerID = m.old_player_id\n",
        "SET r.PlayerID = m.new_player_id;\n",
        "\n",
        "-- Similar updates for Contracts, Appearances, etc.\n",
        "UPDATE Contracts c\n",
        "INNER JOIN player_id_mappings m ON c.PlayerID = m.old_player_id\n",
        "SET c.PlayerID = m.new_player_id;\n",
        "```\n",
        "\n",
        "**Phase 3: Validation**\n",
        "```sql\n",
        "-- Verify no orphaned references\n",
        "SELECT COUNT(*) as orphaned_count\n",
        "FROM Reports r\n",
        "LEFT JOIN Players p ON r.PlayerID = p.PlayerID\n",
        "WHERE p.PlayerID IS NULL;\n",
        "-- Should return 0\n",
        "```\n",
        "\n",
        "### 3.2 Incremental Update Handling\n",
        "\n",
        "For ongoing operations, new data can be processed incrementally:\n",
        "\n",
        "1. **New Player Data**: Run duplicate detection only on new records + existing cleaned data\n",
        "2. **New Reports**: Apply existing ID mappings to new reports before insertion\n",
        "3. **ID Mapping Updates**: Append new mappings to existing mapping table (never overwrite)\n",
        "\n",
        "### 3.3 Rollback Strategy\n",
        "\n",
        "Maintain ability to reverse changes:\n",
        "- Keep original PlayerIDs in audit columns\n",
        "- Store mapping history with timestamps\n",
        "- Enable point-in-time recovery\n",
        "\n",
        "**Example Audit Schema**:\n",
        "```sql\n",
        "ALTER TABLE Reports ADD COLUMN original_player_id VARCHAR(50);\n",
        "ALTER TABLE Reports ADD COLUMN player_id_updated_at TIMESTAMP;\n",
        "```\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## Part 4: Best Practices and Lessons Learned\n",
        "\n",
        "### 4.1 Key Design Decisions\n",
        "\n",
        "1. **Canonical ID Selection**: We prioritize data completeness and CurrentTeam presence when selecting which PlayerID to keep. This ensures we retain the most valuable record.\n",
        "\n",
        "2. **Orphaned Record Handling**: Rather than deleting orphaned records, we create minimal player records from Reporting data. This preserves data lineage and auditability.\n",
        "\n",
        "3. **Mapping Persistence**: JSON format allows easy version control, human readability, and cross-platform compatibility.\n",
        "\n",
        "### 4.2 Performance Considerations\n",
        "\n",
        "- **Lookup Efficiency**: Dictionary/hash map provides O(1) lookup time for ID mappings\n",
        "- **Batch Processing**: Apply mappings to entire DataFrames rather than row-by-row\n",
        "- **Caching**: ID mapping loaded once and reused across all child tables\n",
        "\n",
        "### 4.3 Risk Mitigation\n",
        "\n",
        "- **Validation Gates**: Multiple checkpoints prevent bad data from propagating\n",
        "- **Audit Logging**: Complete trail of all changes for compliance and debugging\n",
        "- **Testing**: Unit tests for mapping logic, integration tests for full pipeline\n",
        "\n",
        "## Conclusion\n",
        "\n",
        "Our implementation demonstrates a production-ready approach to handling PlayerID updates that:\n",
        "\n",
        "✅ **Identifies Affected Tables**: Through foreign key analysis and data profiling  \n",
        "✅ **Maintains Relationships**: Via validation, transaction atomicity, and integrity checks  \n",
        "✅ **Ensures Repeatability**: Through automated ETL pipelines, versioned mappings, and comprehensive logging  \n",
        "\n",
        "This architecture scales from CSV-based analytics to enterprise database systems while maintaining data quality and auditability.\n"
      ]
    }
  ],
  "metadata": {
    "language_info": {
      "name": "python"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 2
}
